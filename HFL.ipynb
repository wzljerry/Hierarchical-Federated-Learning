{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HFL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNykD9d5qVHTW0kwgfdMOez",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wzljerry/Hierarchical-Federated-Learning/blob/main/HFL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewkQ5lek9hgD",
        "outputId": "7bc4133d-e49d-44a3-aae7-650f4481ec62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:70: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:70: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75/75 [==============================] - 9s 103ms/step - loss: 0.5475 - accuracy: 0.9887\n",
            "75/75 [==============================] - 5s 55ms/step - loss: 0.4982 - accuracy: 0.9883\n",
            "75/75 [==============================] - 4s 55ms/step - loss: 0.8647 - accuracy: 0.9737\n",
            "75/75 [==============================] - 4s 54ms/step - loss: 0.2184 - accuracy: 0.9900\n",
            "75/75 [==============================] - 4s 55ms/step - loss: 0.2381 - accuracy: 0.9900\n",
            "75/75 [==============================] - 4s 54ms/step - loss: 1.3441 - accuracy: 0.9450\n",
            "75/75 [==============================] - 7s 89ms/step - loss: 0.8152 - accuracy: 0.9871\n",
            "75/75 [==============================] - 5s 54ms/step - loss: 1.3116 - accuracy: 0.9067\n",
            "75/75 [==============================] - 6s 70ms/step - loss: 0.4423 - accuracy: 0.9883\n",
            "75/75 [==============================] - 6s 62ms/step - loss: 0.5419 - accuracy: 0.9883\n",
            "75/75 [==============================] - 6s 70ms/step - loss: 1.3028 - accuracy: 0.9333\n",
            "75/75 [==============================] - 5s 55ms/step - loss: 0.5037 - accuracy: 0.9887\n",
            "75/75 [==============================] - 5s 55ms/step - loss: 1.3335 - accuracy: 0.9337\n",
            "75/75 [==============================] - 5s 56ms/step - loss: 0.5905 - accuracy: 0.9879\n",
            "75/75 [==============================] - 5s 55ms/step - loss: 0.5661 - accuracy: 0.9879\n",
            "75/75 [==============================] - 5s 55ms/step - loss: 1.2975 - accuracy: 0.9775\n",
            "75/75 [==============================] - 4s 55ms/step - loss: 0.4529 - accuracy: 0.9887\n",
            "75/75 [==============================] - 5s 55ms/step - loss: 1.1508 - accuracy: 0.9554\n",
            "75/75 [==============================] - 5s 56ms/step - loss: 0.8016 - accuracy: 0.9867\n",
            "75/75 [==============================] - 5s 55ms/step - loss: 0.8362 - accuracy: 0.9862\n",
            "75/75 [==============================] - 5s 55ms/step - loss: 1.4460 - accuracy: 0.9379\n",
            "75/75 [==============================] - 5s 61ms/step - loss: 0.5532 - accuracy: 0.9875\n",
            "75/75 [==============================] - 5s 60ms/step - loss: 1.4401 - accuracy: 0.8767\n",
            "75/75 [==============================] - 5s 56ms/step - loss: 0.7662 - accuracy: 0.9871\n",
            "75/75 [==============================] - 6s 70ms/step - loss: 0.7430 - accuracy: 0.9871\n",
            "313/313 [==============================] - 4s 13ms/step - loss: 3.4135 - accuracy: 0.2424\n",
            "Test Loss: 3.413451671600342\n",
            "Test Accuracy: 0.24240000545978546\n",
            "====================================loss================================\n",
            "[3.413451671600342]\n",
            "====================================acc================================\n",
            "[0.24240000545978546]\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)\n",
        "\n",
        "import hashlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from keras.datasets import mnist\n",
        "import keras\n",
        "import random\n",
        "import time\n",
        "from random import choice\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as Ks\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.layers import Convolution2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# to get the training data, and split the data via the number of clients\n",
        "class Get_data:\n",
        "  def __init__(self,n):\n",
        "    self.n=n # number of clients\n",
        "\n",
        "  def load_data(self):\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "  def flatten_data(self,X):\n",
        "    data=[]\n",
        "    for i in X:\n",
        "      da=i.flatten()\n",
        "      data.append(da)\n",
        "    return data\n",
        "\n",
        "  def one_d_to_n_d(self,X):\n",
        "    data=[]\n",
        "    for i in X:\n",
        "      da=i.reshape(28,28)\n",
        "      data.append(da)\n",
        "    return data\n",
        "\n",
        "  def non_iid(self,X,y):\n",
        "    train_X=self.flatten_data(X)\n",
        "    train_y=list(y)\n",
        "    train_data=np.c_[train_X,train_y]\n",
        "    sort_data=train_data[np.argsort(train_data[:,784])]\n",
        "    train_x=sort_data[:,0:784]\n",
        "    train_Y=np.array(sort_data[:,784])\n",
        "    train_x_da=np.array(self.one_d_to_n_d(train_x))\n",
        "    return train_x_da,train_Y\n",
        "\n",
        "  def split_data(self, data, n): \n",
        "    size=int(len(data) / self.n)\n",
        "    s_data = []\n",
        "    for i in range(0, int(len(data)) + 1, size):\n",
        "        c_data = data[i:i + size]\n",
        "        if c_data != []:\n",
        "            s_data.append(c_data)\n",
        "    return s_data\n",
        "\n",
        "  def pre_data(self):\n",
        "\n",
        "    X_train, y_train, X_test, y_test=self.load_data()#iid\n",
        "\n",
        "    X_train,y_train=self.non_iid(X_train,y_train) # non_iid\n",
        "\n",
        "    X_train=self.split_data(X_train,self.n) \n",
        "    y_train=self.split_data(y_train,self.n)\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "class Model:\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def global_models(self):\n",
        "    model= tf.keras.models.Sequential([\n",
        "          tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "          tf.keras.layers.Dense(128, activation='relu'),\n",
        "          tf.keras.layers.Dropout(0.2),\n",
        "          tf.keras.layers.Dense(10, activation='softmax')\n",
        "      ])\n",
        "    return model\n",
        "  \n",
        "  def global_model(self):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(28,28,1))) \n",
        "    model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "  def evaluate_model(self,model,test_X, test_y):\n",
        "    model.compile(optimizer='sgd',\n",
        "                  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "                  metrics=['accuracy'])\n",
        "    score=model.evaluate(test_X, test_y)\n",
        "    print(\"Test Loss:\", score[0])\n",
        "    print('Test Accuracy:', score[1])\n",
        "    return score[0],score[1]\n",
        "\n",
        "class Client:\n",
        "  \n",
        "  def __init__(self,lr,epoch):\n",
        "    #self.n=n\n",
        "    self.epoch=epoch\n",
        "    self.lr = lr\n",
        "    self.loss='categorical_crossentropy'\n",
        "    self.metrics = ['accuracy']\n",
        "    self.optimizer = SGD(lr=self.lr, \n",
        "                decay=self.lr / 2, \n",
        "                momentum=0.9\n",
        "               )\n",
        "  def weight_client(self,data,m,n):\n",
        "    wei_client = []\n",
        "    for i in range(n):\n",
        "        len_data = len(data[i])\n",
        "        proba = len_data / m\n",
        "        wei_client.append(proba)\n",
        "    return wei_client\n",
        "    \n",
        "  \n",
        "  def scale_model_weights(self,weight,scalar,num):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "\n",
        "    fac=scalar[num]\n",
        "\n",
        "    sca=[fac for i in range(steps)]\n",
        "\n",
        "    for i in range(steps):\n",
        "      weight_final.append(sca[i]*weight[i])\n",
        "        #weight_final.append(weight[i])\n",
        "    return weight_final\n",
        "\n",
        "  def training(self,X,y,global_weights):\n",
        "\n",
        "    #fact=self.weight_client()\n",
        "    model=Model().global_model()\n",
        "\n",
        "    model.compile(optimizer=self.optimizer,\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    model.set_weights(global_weights)\n",
        "    model.fit(X,y,epochs=self.epoch)\n",
        "    weights=model.get_weights()\n",
        "\n",
        "    return weights\n",
        "\n",
        "class Server:\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def scale_model_weights(self,weight,scalar,num):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    wei=[]\n",
        "    steps = 8\n",
        "\n",
        "    for j in range(num):\n",
        "      fa=scalar[j]\n",
        "      scar=[fa for k in range(steps)]\n",
        "      for i in range(steps):\n",
        "        weight_final.append(scar[i]*weight[j][i])\n",
        "        #weight_final.append(weight[i])\n",
        "      wei.append(weight_final)\n",
        "    return wei\n",
        "\n",
        "  def sum_scaled_weights(self,scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "    return avg_grad\n",
        "\n",
        "  def evaluate(self,model,test_X, test_y):\n",
        "    loss,acc=Model.evaluate_model(model,test_X, test_y)\n",
        "    return loss,acc\n",
        "\n",
        "class compute_estimated_weights:\n",
        "  def __init__(self,weight):\n",
        "    self.weight=weight\n",
        "\n",
        "  def sum_scaled_weights(self,scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "    return avg_grad\n",
        "\n",
        "  def scale_model_weights(self,weight,scalar):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "    for i in range(steps):\n",
        "      weight_final.append(scalar[i]*weight[i])\n",
        "    return weight_final\n",
        "\n",
        "  def get_difference_single_client(self):\n",
        "    scalar=[-1 for i in range(100)]#len(self.weight)\n",
        "    wei_d=[]\n",
        "    for i in range(len(self.weight)):\n",
        "      if i+1>=len(self.weight):\n",
        "        pass\n",
        "      else:\n",
        "        sw=self.scale_model_weights(self.weight[i],scalar)\n",
        "        weit_c=[self.weight[i],self.weight[i+1]]\n",
        "        df=self.sum_scaled_weights(weit_c)\n",
        "        wei_d.append(df)\n",
        "    return wei_d\n",
        "  \n",
        "  def get_mean_difference_single_client(self,weight):\n",
        "    scalar=[1/len(weight) for i in range(100)]#len(weight)\n",
        "    wei=[]\n",
        "    for i in range(len(weight)):\n",
        "      sw=self.scale_model_weights(weight[i],scalar)\n",
        "      wei.append(sw)\n",
        "    #print(len(wei))\n",
        "    mean_df=self.sum_scaled_weights(wei)\n",
        "    return mean_df\n",
        "\n",
        "  def get_estimated_weights_single_client(self):\n",
        "    weit=self.get_difference_single_client()\n",
        "    #wei=self.get_mean_difference_single_client(weit)\n",
        "    mean_weight=self.get_mean_difference_single_client(weit)\n",
        "    wei_c=[self.weight[-1],mean_weight]\n",
        "    estimated_weights=self.sum_scaled_weights(wei_c)\n",
        "    return estimated_weights\n",
        "\n",
        "def reshape_his_wei(data,T,server,client):\n",
        "  W=[]\n",
        "  X=cut([i for i in data],int(len(data)/(T)))\n",
        "  for j in range(len(X)):\n",
        "    Y=cut([i for i in X[j]],int(len(X[j])/num_server)) #每个t有2个server\n",
        "    Z=cut([i for i in Y[server]],int(len(Y[server])/K)) #这个server有2个k\n",
        "    for h in range(len(Z)): # 进入每个k\n",
        "      wei=Z[h][client]\n",
        "      W.append(wei)\n",
        "  return W\n",
        "\n",
        "def reshape_egde_wei(data,num_server,t,j):\n",
        "  w=[]\n",
        "  X=X=cut([i for i in data],num_server)\n",
        "  for i in range(len(X)):\n",
        "    w.append(X[i][j])\n",
        "  return w\n",
        "\n",
        "def reshape_local_wei(data,number_client,num):\n",
        "  w=[]\n",
        "  X=cut([i for i in data],num_client)\n",
        "  for i in range(len(X)):\n",
        "    w.append(X[i][num])\n",
        "  return w\n",
        "\n",
        "def random_stragglers(t,j,i,k,t_s,j_s,i_s,k_s):\n",
        "  a=False\n",
        "  if t in t_s and j in j_s and i in i_s and k in k_s:\n",
        "    a=True\n",
        "    return a\n",
        "\n",
        "def t_round(t,j,t_s,j_s):\n",
        "  a=False\n",
        "  if t in t_s and j in j_s:\n",
        "    a=True\n",
        "    return a\n",
        "\n",
        "def get_order(num_server,num_client):\n",
        "  w=[]\n",
        "  h=[]\n",
        "  X=[i for i in range(num_client*num_server)]\n",
        "  X=np.array(X)\n",
        "  y=X.reshape(num_server,num_client)\n",
        "  y=pd.DataFrame(y)\n",
        "  for i in range(num_client):\n",
        "    d=list(y.iloc[:,i])\n",
        "    w.append(d)\n",
        "  k=list(w)\n",
        "  \n",
        "  for j in range(num_server):\n",
        "    h.extend(k[j])\n",
        "  return h\n",
        "\n",
        "def cut(obj, sec):\n",
        "  data=[obj[i:i+sec] for i in range(0,len(obj),sec)]\n",
        "  return data\n",
        "\n",
        "def shuffle_data(data):\n",
        "  for i in range(len(data)):\n",
        "    if (i%2==0) and i<=(len(data)/2-1):\n",
        "      data[i],data[-i]=data[-i],data[i]\n",
        "  return data\n",
        "\n",
        "def factor_re(factor,num,i,t,k):\n",
        "  factor[i]=(0.80**(t))*factor[i]\n",
        "  #factor[i]=(1/(1+0.001*t))*factor[i]\n",
        "  for j in range(len(factor)):\n",
        "    if j != i:\n",
        "      factor[j]=(1-factor[i])/(len(factor)-num)\n",
        "  return factor\n",
        "\n",
        "def decay_weights(data,t):\n",
        "    data=(0.80**(t))*data\n",
        "    return data\n",
        "\n",
        "def HFL(num_server,K,num_client,T,lr,epoch):\n",
        "\n",
        "  client=Client(lr,epoch)\n",
        "\n",
        "  server=Server()\n",
        "  global_model=Model().global_model()\n",
        "\n",
        "  get_data=Get_data(num_server*num_client)\n",
        "  train_X,train_y, test_X, test_y=get_data.load_data()\n",
        "  X_train, y_train, X_test, y_test=get_data.pre_data()\n",
        "  m=len(train_X)\n",
        "\n",
        "  X=cut([i for i in X_train],num_client)\n",
        "  y=cut([i for i in y_train],num_client)\n",
        "\n",
        "  accuracy=[]\n",
        "  losses=[]\n",
        "  server_w=[]\n",
        "  factor_s=[1/num_server for l in range(num_server)]\n",
        "  fa=[1/num_client for h in range(10)]\n",
        "  \n",
        "  for t in range(T):\n",
        "    global_weights=global_model.get_weights()\n",
        "    server_w=[]\n",
        "    for j in range(num_server):\n",
        "      local_weights=global_weights\n",
        "      for k in range(K):\n",
        "        weit=[]\n",
        "        for i in range(num_client):\n",
        "          weix=client.training(X[j][i],y[j][i],local_weights)\n",
        "          weix_s=client.scale_model_weights(weix, fa,i)\n",
        "          weit.append(weix_s)\n",
        "        local_w_k=server.sum_scaled_weights(weit)\n",
        "        local_weights=local_w_k\n",
        "      server_weights=client.scale_model_weights(local_w_k,factor_s,j)\n",
        "      server_w.append(server_weights)\n",
        "    global_weight=server.sum_scaled_weights(server_w) # fedavg\n",
        "    global_model.set_weights(global_weight)\n",
        "    loss,acc=Model().evaluate_model(global_model,test_X,test_y)\n",
        "    losses.append(loss)\n",
        "    accuracy.append(acc)\n",
        "    \n",
        "  return losses,accuracy\n",
        "\n",
        "if __name__=='__main__':\n",
        "  num_server=5# number of servers\n",
        "  K=1 # number of local rounds\n",
        "  T=50 # number of global rounds\n",
        "  num_client=5# number of clients for each server\n",
        "  lr=0.001 # learning rate\n",
        "  epoch=1 # local iterations\n",
        "\n",
        "  loss,acc=HFL(num_server,K,num_client,T,lr,epoch)\n",
        "  loss\n",
        "  print('====================================loss================================')\n",
        "  print(loss)\n",
        "  print('====================================acc================================')\n",
        "  print(acc)"
      ]
    }
  ]
}