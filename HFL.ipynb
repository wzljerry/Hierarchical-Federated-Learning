{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HFL.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPK7RBk/hg/pqABO3agIxcQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wzljerry/Hierarchical-Federated-Learning/blob/main/HFL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewkQ5lek9hgD",
        "outputId": "4c1d5c3c-8b09-4d2f-9ebb-569879492ac1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1768: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:70: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:70: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75/75 [==============================] - 5s 57ms/step - loss: 0.5017 - accuracy: 0.9875\n",
            "75/75 [==============================] - 5s 58ms/step - loss: 0.5292 - accuracy: 0.9871\n",
            "75/75 [==============================] - 5s 55ms/step - loss: 0.8208 - accuracy: 0.9654\n",
            "75/75 [==============================] - 4s 54ms/step - loss: 0.5326 - accuracy: 0.9867\n",
            "75/75 [==============================] - 4s 55ms/step - loss: 0.4916 - accuracy: 0.9871\n",
            "75/75 [==============================] - 4s 54ms/step - loss: 1.4135 - accuracy: 0.9246\n",
            "75/75 [==============================] - 5s 55ms/step - loss: 1.1340 - accuracy: 0.9871\n",
            "75/75 [==============================] - 4s 55ms/step - loss: 1.8996 - accuracy: 0.8537\n",
            "75/75 [==============================] - 4s 54ms/step - loss: 0.4720 - accuracy: 0.9879\n",
            "52/75 [===================>..........] - ETA: 1s - loss: 0.5402 - accuracy: 0.9856"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)\n",
        "\n",
        "import hashlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from keras.datasets import mnist\n",
        "import keras\n",
        "import random\n",
        "import time\n",
        "from random import choice\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as Ks\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.layers import Convolution2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# to get the training data, and split the data via the number of clients\n",
        "class Get_data:\n",
        "  def __init__(self,n):\n",
        "    self.n=n # number of clients\n",
        "\n",
        "  def load_data(self):\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "  def flatten_data(self,X):\n",
        "    data=[]\n",
        "    for i in X:\n",
        "      da=i.flatten()\n",
        "      data.append(da)\n",
        "    return data\n",
        "\n",
        "  def one_d_to_n_d(self,X):\n",
        "    data=[]\n",
        "    for i in X:\n",
        "      da=i.reshape(28,28)\n",
        "      data.append(da)\n",
        "    return data\n",
        "\n",
        "  def non_iid(self,X,y):\n",
        "    train_X=self.flatten_data(X)\n",
        "    train_y=list(y)\n",
        "    train_data=np.c_[train_X,train_y]\n",
        "    sort_data=train_data[np.argsort(train_data[:,784])]\n",
        "    train_x=sort_data[:,0:784]\n",
        "    train_Y=np.array(sort_data[:,784])\n",
        "    train_x_da=np.array(self.one_d_to_n_d(train_x))\n",
        "    return train_x_da,train_Y\n",
        "\n",
        "  def split_data(self, data, n): \n",
        "    size=int(len(data) / self.n)\n",
        "    s_data = []\n",
        "    for i in range(0, int(len(data)) + 1, size):\n",
        "        c_data = data[i:i + size]\n",
        "        if c_data != []:\n",
        "            s_data.append(c_data)\n",
        "    return s_data\n",
        "\n",
        "  def pre_data(self):\n",
        "\n",
        "    X_train, y_train, X_test, y_test=self.load_data()#iid\n",
        "\n",
        "    X_train,y_train=self.non_iid(X_train,y_train) # non_iid\n",
        "\n",
        "    X_train=self.split_data(X_train,self.n) \n",
        "    y_train=self.split_data(y_train,self.n)\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "class Model:\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def global_models(self):\n",
        "    model= tf.keras.models.Sequential([\n",
        "          tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "          tf.keras.layers.Dense(128, activation='relu'),\n",
        "          tf.keras.layers.Dropout(0.2),\n",
        "          tf.keras.layers.Dense(10, activation='softmax')\n",
        "      ])\n",
        "    return model\n",
        "  \n",
        "  def global_model(self):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(28,28,1))) \n",
        "    model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "  def evaluate_model(self,model,test_X, test_y):\n",
        "    model.compile(optimizer='sgd',\n",
        "                  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "                  metrics=['accuracy'])\n",
        "    score=model.evaluate(test_X, test_y)\n",
        "    print(\"Test Loss:\", score[0])\n",
        "    print('Test Accuracy:', score[1])\n",
        "    return score[0],score[1]\n",
        "\n",
        "class Client:\n",
        "  \n",
        "  def __init__(self,lr,epoch):\n",
        "    #self.n=n\n",
        "    self.epoch=epoch\n",
        "    self.lr = lr\n",
        "    self.loss='categorical_crossentropy'\n",
        "    self.metrics = ['accuracy']\n",
        "    self.optimizer = SGD(lr=self.lr, \n",
        "                decay=self.lr / 2, \n",
        "                momentum=0.9\n",
        "               )\n",
        "  def weight_client(self,data,m,n):\n",
        "    wei_client = []\n",
        "    for i in range(n):\n",
        "        len_data = len(data[i])\n",
        "        proba = len_data / m\n",
        "        wei_client.append(proba)\n",
        "    return wei_client\n",
        "    \n",
        "  \n",
        "  def scale_model_weights(self,weight,scalar,num):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "\n",
        "    fac=scalar[num]\n",
        "\n",
        "    sca=[fac for i in range(steps)]\n",
        "\n",
        "    for i in range(steps):\n",
        "      weight_final.append(sca[i]*weight[i])\n",
        "        #weight_final.append(weight[i])\n",
        "    return weight_final\n",
        "\n",
        "  def training(self,X,y,global_weights):\n",
        "\n",
        "    #fact=self.weight_client()\n",
        "    model=Model().global_model()\n",
        "\n",
        "    model.compile(optimizer=self.optimizer,\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    model.set_weights(global_weights)\n",
        "    model.fit(X,y,epochs=self.epoch)\n",
        "    weights=model.get_weights()\n",
        "\n",
        "    return weights\n",
        "\n",
        "class Server:\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def scale_model_weights(self,weight,scalar,num):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    wei=[]\n",
        "    steps = 8\n",
        "\n",
        "    for j in range(num):\n",
        "      fa=scalar[j]\n",
        "      scar=[fa for k in range(steps)]\n",
        "      for i in range(steps):\n",
        "        weight_final.append(scar[i]*weight[j][i])\n",
        "        #weight_final.append(weight[i])\n",
        "      wei.append(weight_final)\n",
        "    return wei\n",
        "\n",
        "  def sum_scaled_weights(self,scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "    return avg_grad\n",
        "\n",
        "  def evaluate(self,model,test_X, test_y):\n",
        "    loss,acc=Model.evaluate_model(model,test_X, test_y)\n",
        "    return loss,acc\n",
        "\n",
        "def HFL(num_server,K,num_client,T,lr,epoch):\n",
        "\n",
        "  client=Client(lr,epoch)\n",
        "\n",
        "  server=Server()\n",
        "  global_model=Model().global_model()\n",
        "\n",
        "  get_data=Get_data(num_server*num_client)\n",
        "  train_X,train_y, test_X, test_y=get_data.load_data()\n",
        "  X_train, y_train, X_test, y_test=get_data.pre_data()\n",
        "  m=len(train_X)\n",
        "\n",
        "  X=cut([i for i in X_train],num_client)\n",
        "  y=cut([i for i in y_train],num_client)\n",
        "\n",
        "  accuracy=[]\n",
        "  losses=[]\n",
        "  server_w=[]\n",
        "  factor_s=[1/num_server for l in range(num_server)]\n",
        "  fa=[1/num_client for h in range(10)]\n",
        "  \n",
        "  for t in range(T):\n",
        "    global_weights=global_model.get_weights()\n",
        "    server_w=[]\n",
        "    for j in range(num_server):\n",
        "      local_weights=global_weights\n",
        "      for k in range(K):\n",
        "        weit=[]\n",
        "        for i in range(num_client):\n",
        "          weix=client.training(X[j][i],y[j][i],local_weights)\n",
        "          weix_s=client.scale_model_weights(weix, fa,i)\n",
        "          weit.append(weix_s)\n",
        "        local_w_k=server.sum_scaled_weights(weit)\n",
        "        local_weights=local_w_k\n",
        "      server_weights=client.scale_model_weights(local_w_k,factor_s,j)\n",
        "      server_w.append(server_weights)\n",
        "    global_weight=server.sum_scaled_weights(server_w) # fedavg\n",
        "    global_model.set_weights(global_weight)\n",
        "    loss,acc=Model().evaluate_model(global_model,test_X,test_y)\n",
        "    losses.append(loss)\n",
        "    accuracy.append(acc)\n",
        "    \n",
        "  return losses,accuracy\n",
        "\n",
        "if __name__=='__main__':\n",
        "  num_server=5# number of servers\n",
        "  K=1 # number of local rounds\n",
        "  T=50 # number of global rounds\n",
        "  num_client=5# number of clients for each server\n",
        "  lr=0.001 # learning rate\n",
        "  epoch=1 # local iterations\n",
        "\n",
        "  loss,acc=HFL(num_server,K,num_client,T,lr,epoch)\n",
        "  loss\n",
        "  print('====================================loss================================')\n",
        "  print(loss)\n",
        "  print('====================================acc================================')\n",
        "  print(acc)"
      ]
    }
  ]
}