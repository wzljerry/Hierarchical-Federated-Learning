{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMql3FtcVp1A7211PaOiY6D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wzljerry/Hierarchical-Federated-Learning/blob/main/FL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GhRZ5pq57qx",
        "outputId": "8a37495a-8a96-4c8d-948c-e405dd4b015f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1768: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:70: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:70: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "188/188 [==============================] - 1s 3ms/step - loss: 0.7104 - accuracy: 0.9890\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1517 - accuracy: 0.9953\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.6682 - accuracy: 0.9730\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.5830 - accuracy: 0.9473\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.7113 - accuracy: 0.9698\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.7234 - accuracy: 0.9675\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.4354 - accuracy: 0.9852\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1643 - accuracy: 0.9955\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.7239 - accuracy: 0.9748\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.6646 - accuracy: 0.9842\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 2.7470 - accuracy: 0.1419\n",
            "Test Loss: 2.7470273971557617\n",
            "Test Accuracy: 0.14190000295639038\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0473 - accuracy: 0.9867\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0485 - accuracy: 0.9825\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1747 - accuracy: 0.9487\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.3167 - accuracy: 0.8998\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.2110 - accuracy: 0.9437\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1939 - accuracy: 0.9470\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1603 - accuracy: 0.9608\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0838 - accuracy: 0.9748\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1629 - accuracy: 0.9567\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1129 - accuracy: 0.9755\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 2.0430 - accuracy: 0.3272\n",
            "Test Loss: 2.0429537296295166\n",
            "Test Accuracy: 0.3271999955177307\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1126 - accuracy: 0.9667\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0432 - accuracy: 0.9867\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1566 - accuracy: 0.9527\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.2594 - accuracy: 0.9178\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1402 - accuracy: 0.9590\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1909 - accuracy: 0.9393\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1217 - accuracy: 0.9720\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0304 - accuracy: 0.9905\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1438 - accuracy: 0.9633\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.2037 - accuracy: 0.9477\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.7232 - accuracy: 0.4313\n",
            "Test Loss: 1.7231712341308594\n",
            "Test Accuracy: 0.43130001425743103\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0648 - accuracy: 0.9813\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0255 - accuracy: 0.9933\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1244 - accuracy: 0.9628\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.2307 - accuracy: 0.9252\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1368 - accuracy: 0.9582\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1911 - accuracy: 0.9393\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1282 - accuracy: 0.9645\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0520 - accuracy: 0.9837\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1472 - accuracy: 0.9540\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1745 - accuracy: 0.9587\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.4363 - accuracy: 0.5599\n",
            "Test Loss: 1.4362525939941406\n",
            "Test Accuracy: 0.5598999857902527\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0589 - accuracy: 0.9832\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0263 - accuracy: 0.9935\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1242 - accuracy: 0.9615\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.2191 - accuracy: 0.9317\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1130 - accuracy: 0.9670\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1728 - accuracy: 0.9443\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1083 - accuracy: 0.9693\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0362 - accuracy: 0.9887\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1276 - accuracy: 0.9630\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1617 - accuracy: 0.9627\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.2688 - accuracy: 0.6098\n",
            "Test Loss: 1.2687718868255615\n",
            "Test Accuracy: 0.6097999811172485\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0465 - accuracy: 0.9852\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0278 - accuracy: 0.9927\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1190 - accuracy: 0.9645\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.2116 - accuracy: 0.9333\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1138 - accuracy: 0.9647\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1721 - accuracy: 0.9478\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0973 - accuracy: 0.9725\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0380 - accuracy: 0.9872\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1319 - accuracy: 0.9622\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1426 - accuracy: 0.9667\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.1393 - accuracy: 0.6502\n",
            "Test Loss: 1.1392704248428345\n",
            "Test Accuracy: 0.6502000093460083\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0549 - accuracy: 0.9833\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0257 - accuracy: 0.9933\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1192 - accuracy: 0.9640\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.2060 - accuracy: 0.9332\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1073 - accuracy: 0.9658\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1615 - accuracy: 0.9500\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1046 - accuracy: 0.9755\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0372 - accuracy: 0.9870\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1244 - accuracy: 0.9622\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1508 - accuracy: 0.9660\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.0770 - accuracy: 0.6843\n",
            "Test Loss: 1.077004313468933\n",
            "Test Accuracy: 0.6843000054359436\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0499 - accuracy: 0.9847\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0237 - accuracy: 0.9948\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1036 - accuracy: 0.9683\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1993 - accuracy: 0.9387\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1137 - accuracy: 0.9662\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1710 - accuracy: 0.9495\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0958 - accuracy: 0.9733\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0417 - accuracy: 0.9867\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1311 - accuracy: 0.9607\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1374 - accuracy: 0.9663\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9601 - accuracy: 0.6980\n",
            "Test Loss: 0.9600915908813477\n",
            "Test Accuracy: 0.6980000138282776\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0446 - accuracy: 0.9855\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0245 - accuracy: 0.9930\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1122 - accuracy: 0.9645\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1989 - accuracy: 0.9360\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0999 - accuracy: 0.9693\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1487 - accuracy: 0.9503\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0897 - accuracy: 0.9748\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0424 - accuracy: 0.9858\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1328 - accuracy: 0.9623\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1434 - accuracy: 0.9655\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.9041 - accuracy: 0.7253\n",
            "Test Loss: 0.9040852785110474\n",
            "Test Accuracy: 0.7253000140190125\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0453 - accuracy: 0.9850\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0264 - accuracy: 0.9942\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1052 - accuracy: 0.9673\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1930 - accuracy: 0.9382\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1113 - accuracy: 0.9700\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1597 - accuracy: 0.9488\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0978 - accuracy: 0.9725\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0419 - accuracy: 0.9863\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1330 - accuracy: 0.9585\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1342 - accuracy: 0.9680\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8449 - accuracy: 0.7480\n",
            "Test Loss: 0.844890832901001\n",
            "Test Accuracy: 0.7480000257492065\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0444 - accuracy: 0.9867\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0278 - accuracy: 0.9925\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1095 - accuracy: 0.9667\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1944 - accuracy: 0.9377\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1183 - accuracy: 0.9643\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1432 - accuracy: 0.9562\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0929 - accuracy: 0.9760\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0492 - accuracy: 0.9853\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1225 - accuracy: 0.9640\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1383 - accuracy: 0.9687\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8153 - accuracy: 0.7575\n",
            "Test Loss: 0.8153239488601685\n",
            "Test Accuracy: 0.7574999928474426\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0450 - accuracy: 0.9848\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0323 - accuracy: 0.9922\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0990 - accuracy: 0.9702\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1850 - accuracy: 0.9382\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1144 - accuracy: 0.9667\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1580 - accuracy: 0.9517\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0900 - accuracy: 0.9743\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0445 - accuracy: 0.9863\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1248 - accuracy: 0.9605\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1391 - accuracy: 0.9680\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.7818 - accuracy: 0.7668\n",
            "Test Loss: 0.781760573387146\n",
            "Test Accuracy: 0.7667999863624573\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0446 - accuracy: 0.9850\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0303 - accuracy: 0.9925\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1070 - accuracy: 0.9673\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1871 - accuracy: 0.9393\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1092 - accuracy: 0.9693\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1455 - accuracy: 0.9522\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0911 - accuracy: 0.9732\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0448 - accuracy: 0.9852\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1317 - accuracy: 0.9590\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1302 - accuracy: 0.9687\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.7467 - accuracy: 0.7771\n",
            "Test Loss: 0.7467422485351562\n",
            "Test Accuracy: 0.7771000266075134\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0479 - accuracy: 0.9848\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0301 - accuracy: 0.9922\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1066 - accuracy: 0.9687\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1891 - accuracy: 0.9407\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1093 - accuracy: 0.9662\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1493 - accuracy: 0.9527\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0967 - accuracy: 0.9738\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0536 - accuracy: 0.9838\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1299 - accuracy: 0.9573\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1371 - accuracy: 0.9643\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.7325 - accuracy: 0.7837\n",
            "Test Loss: 0.7324679493904114\n",
            "Test Accuracy: 0.7836999893188477\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0462 - accuracy: 0.9853\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0309 - accuracy: 0.9915\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1033 - accuracy: 0.9708\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1802 - accuracy: 0.9398\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1050 - accuracy: 0.9670\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1577 - accuracy: 0.9518\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0892 - accuracy: 0.9737\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0522 - accuracy: 0.9848\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1295 - accuracy: 0.9555\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1349 - accuracy: 0.9687\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.7064 - accuracy: 0.7880\n",
            "Test Loss: 0.7064457535743713\n",
            "Test Accuracy: 0.7879999876022339\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0440 - accuracy: 0.9863\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0313 - accuracy: 0.9937\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1043 - accuracy: 0.9707\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1826 - accuracy: 0.9372\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1142 - accuracy: 0.9658\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1465 - accuracy: 0.9568\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0906 - accuracy: 0.9733\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0521 - accuracy: 0.9838\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1288 - accuracy: 0.9603\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1335 - accuracy: 0.9677\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.6792 - accuracy: 0.7970\n",
            "Test Loss: 0.679190456867218\n",
            "Test Accuracy: 0.796999990940094\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0507 - accuracy: 0.9847\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0312 - accuracy: 0.9917\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1003 - accuracy: 0.9715\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1774 - accuracy: 0.9438\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1083 - accuracy: 0.9642\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1592 - accuracy: 0.9503\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0888 - accuracy: 0.9760\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0482 - accuracy: 0.9837\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1299 - accuracy: 0.9600\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1315 - accuracy: 0.9660\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.6538 - accuracy: 0.8051\n",
            "Test Loss: 0.6537548899650574\n",
            "Test Accuracy: 0.8051000237464905\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0512 - accuracy: 0.9828\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0347 - accuracy: 0.9908\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0993 - accuracy: 0.9695\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1854 - accuracy: 0.9385\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1170 - accuracy: 0.9647\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1572 - accuracy: 0.9497\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0919 - accuracy: 0.9738\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0512 - accuracy: 0.9823\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1238 - accuracy: 0.9612\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1372 - accuracy: 0.9667\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.6437 - accuracy: 0.8142\n",
            "Test Loss: 0.6437087655067444\n",
            "Test Accuracy: 0.8141999840736389\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0440 - accuracy: 0.9858\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0328 - accuracy: 0.9905\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0965 - accuracy: 0.9723\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1787 - accuracy: 0.9428\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1068 - accuracy: 0.9665\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1461 - accuracy: 0.9537\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0872 - accuracy: 0.9740\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0517 - accuracy: 0.9828\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1346 - accuracy: 0.9590\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1255 - accuracy: 0.9690\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.6333 - accuracy: 0.8066\n",
            "Test Loss: 0.6333378553390503\n",
            "Test Accuracy: 0.8065999746322632\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0489 - accuracy: 0.9848\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0394 - accuracy: 0.9888\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1046 - accuracy: 0.9675\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1865 - accuracy: 0.9397\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1024 - accuracy: 0.9705\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1475 - accuracy: 0.9553\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0932 - accuracy: 0.9737\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0501 - accuracy: 0.9845\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1287 - accuracy: 0.9558\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1377 - accuracy: 0.9657\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.6169 - accuracy: 0.8172\n",
            "Test Loss: 0.6169330477714539\n",
            "Test Accuracy: 0.8172000050544739\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0540 - accuracy: 0.9840\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0351 - accuracy: 0.9927\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0956 - accuracy: 0.9707\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1798 - accuracy: 0.9423\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1156 - accuracy: 0.9645\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1549 - accuracy: 0.9525\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0923 - accuracy: 0.9743\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0564 - accuracy: 0.9830\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1458 - accuracy: 0.9542\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1305 - accuracy: 0.9690\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.6078 - accuracy: 0.8214\n",
            "Test Loss: 0.6077547073364258\n",
            "Test Accuracy: 0.821399986743927\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0518 - accuracy: 0.9847\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0345 - accuracy: 0.9903\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0937 - accuracy: 0.9712\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1683 - accuracy: 0.9442\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1082 - accuracy: 0.9667\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1567 - accuracy: 0.9533\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0854 - accuracy: 0.9758\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0552 - accuracy: 0.9813\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1348 - accuracy: 0.9577\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1284 - accuracy: 0.9668\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5915 - accuracy: 0.8276\n",
            "Test Loss: 0.5914746522903442\n",
            "Test Accuracy: 0.8276000022888184\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0470 - accuracy: 0.9848\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0336 - accuracy: 0.9905\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1001 - accuracy: 0.9702\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1748 - accuracy: 0.9443\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1152 - accuracy: 0.9657\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1500 - accuracy: 0.9538\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0971 - accuracy: 0.9725\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0534 - accuracy: 0.9848\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1432 - accuracy: 0.9538\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1250 - accuracy: 0.9683\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5849 - accuracy: 0.8275\n",
            "Test Loss: 0.5848876237869263\n",
            "Test Accuracy: 0.8274999856948853\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0442 - accuracy: 0.9843\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0334 - accuracy: 0.9917\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1004 - accuracy: 0.9682\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1806 - accuracy: 0.9440\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1084 - accuracy: 0.9685\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1546 - accuracy: 0.9542\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0964 - accuracy: 0.9732\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0574 - accuracy: 0.9832\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1333 - accuracy: 0.9582\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1246 - accuracy: 0.9653\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5755 - accuracy: 0.8269\n",
            "Test Loss: 0.5755411386489868\n",
            "Test Accuracy: 0.8269000053405762\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0518 - accuracy: 0.9827\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0395 - accuracy: 0.9888\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0986 - accuracy: 0.9697\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1697 - accuracy: 0.9430\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1183 - accuracy: 0.9628\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1481 - accuracy: 0.9573\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0887 - accuracy: 0.9742\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0538 - accuracy: 0.9820\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1256 - accuracy: 0.9595\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1334 - accuracy: 0.9657\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5642 - accuracy: 0.8338\n",
            "Test Loss: 0.5641991496086121\n",
            "Test Accuracy: 0.8338000178337097\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0466 - accuracy: 0.9850\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0371 - accuracy: 0.9898\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1039 - accuracy: 0.9670\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1833 - accuracy: 0.9417\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1186 - accuracy: 0.9635\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1577 - accuracy: 0.9492\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0950 - accuracy: 0.9738\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0601 - accuracy: 0.9797\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1404 - accuracy: 0.9577\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1284 - accuracy: 0.9672\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5571 - accuracy: 0.8360\n",
            "Test Loss: 0.5570924282073975\n",
            "Test Accuracy: 0.8360000252723694\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0434 - accuracy: 0.9870\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0394 - accuracy: 0.9905\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0970 - accuracy: 0.9702\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1694 - accuracy: 0.9438\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1108 - accuracy: 0.9665\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1534 - accuracy: 0.9520\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0946 - accuracy: 0.9722\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0583 - accuracy: 0.9810\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1342 - accuracy: 0.9557\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1358 - accuracy: 0.9682\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5518 - accuracy: 0.8360\n",
            "Test Loss: 0.5518342852592468\n",
            "Test Accuracy: 0.8360000252723694\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0476 - accuracy: 0.9860\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0413 - accuracy: 0.9892\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1017 - accuracy: 0.9697\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1725 - accuracy: 0.9403\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1172 - accuracy: 0.9632\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1589 - accuracy: 0.9500\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0965 - accuracy: 0.9720\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0641 - accuracy: 0.9812\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1267 - accuracy: 0.9568\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1312 - accuracy: 0.9653\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5427 - accuracy: 0.8413\n",
            "Test Loss: 0.5426708459854126\n",
            "Test Accuracy: 0.8413000106811523\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0500 - accuracy: 0.9843\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0350 - accuracy: 0.9912\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0966 - accuracy: 0.9682\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1631 - accuracy: 0.9473\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1245 - accuracy: 0.9630\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1624 - accuracy: 0.9507\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0866 - accuracy: 0.9738\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0557 - accuracy: 0.9800\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1385 - accuracy: 0.9570\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1214 - accuracy: 0.9685\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5354 - accuracy: 0.8407\n",
            "Test Loss: 0.5353856682777405\n",
            "Test Accuracy: 0.8406999707221985\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0546 - accuracy: 0.9833\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0385 - accuracy: 0.9892\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0988 - accuracy: 0.9710\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1758 - accuracy: 0.9383\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1195 - accuracy: 0.9652\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1552 - accuracy: 0.9533\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0952 - accuracy: 0.9723\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0641 - accuracy: 0.9800\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1346 - accuracy: 0.9583\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1291 - accuracy: 0.9657\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5329 - accuracy: 0.8436\n",
            "Test Loss: 0.5328880548477173\n",
            "Test Accuracy: 0.8435999751091003\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0473 - accuracy: 0.9847\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0450 - accuracy: 0.9875\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0928 - accuracy: 0.9727\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1616 - accuracy: 0.9455\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1124 - accuracy: 0.9668\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1622 - accuracy: 0.9495\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0931 - accuracy: 0.9723\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0649 - accuracy: 0.9810\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1500 - accuracy: 0.9547\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1234 - accuracy: 0.9662\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5278 - accuracy: 0.8461\n",
            "Test Loss: 0.5277693867683411\n",
            "Test Accuracy: 0.8460999727249146\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0547 - accuracy: 0.9835\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0417 - accuracy: 0.9903\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0951 - accuracy: 0.9713\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1679 - accuracy: 0.9417\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1151 - accuracy: 0.9662\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1553 - accuracy: 0.9517\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0993 - accuracy: 0.9718\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0597 - accuracy: 0.9797\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1468 - accuracy: 0.9530\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1287 - accuracy: 0.9648\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5279 - accuracy: 0.8448\n",
            "Test Loss: 0.5278629660606384\n",
            "Test Accuracy: 0.8447999954223633\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0536 - accuracy: 0.9838\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0417 - accuracy: 0.9902\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0940 - accuracy: 0.9707\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1729 - accuracy: 0.9408\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1145 - accuracy: 0.9648\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1485 - accuracy: 0.9550\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0901 - accuracy: 0.9723\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0633 - accuracy: 0.9802\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1403 - accuracy: 0.9563\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1228 - accuracy: 0.9682\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5160 - accuracy: 0.8486\n",
            "Test Loss: 0.5159521698951721\n",
            "Test Accuracy: 0.8485999703407288\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0484 - accuracy: 0.9845\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0390 - accuracy: 0.9905\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0964 - accuracy: 0.9698\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1693 - accuracy: 0.9453\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1242 - accuracy: 0.9622\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1661 - accuracy: 0.9495\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0960 - accuracy: 0.9733\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0645 - accuracy: 0.9783\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1405 - accuracy: 0.9553\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1206 - accuracy: 0.9678\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5144 - accuracy: 0.8476\n",
            "Test Loss: 0.5143879055976868\n",
            "Test Accuracy: 0.847599983215332\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0543 - accuracy: 0.9838\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0448 - accuracy: 0.9878\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0863 - accuracy: 0.9722\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1607 - accuracy: 0.9468\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1179 - accuracy: 0.9665\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1516 - accuracy: 0.9512\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0962 - accuracy: 0.9733\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0612 - accuracy: 0.9803\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1402 - accuracy: 0.9552\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1311 - accuracy: 0.9668\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5029 - accuracy: 0.8530\n",
            "Test Loss: 0.5029312372207642\n",
            "Test Accuracy: 0.8529999852180481\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0484 - accuracy: 0.9847\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0396 - accuracy: 0.9912\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0990 - accuracy: 0.9732\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1723 - accuracy: 0.9435\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1224 - accuracy: 0.9608\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1628 - accuracy: 0.9478\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1000 - accuracy: 0.9710\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0644 - accuracy: 0.9797\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1352 - accuracy: 0.9557\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1254 - accuracy: 0.9665\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5062 - accuracy: 0.8509\n",
            "Test Loss: 0.5061954259872437\n",
            "Test Accuracy: 0.8508999943733215\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0563 - accuracy: 0.9832\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0418 - accuracy: 0.9887\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1003 - accuracy: 0.9703\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1643 - accuracy: 0.9443\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1187 - accuracy: 0.9643\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1604 - accuracy: 0.9513\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0996 - accuracy: 0.9695\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0681 - accuracy: 0.9800\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1405 - accuracy: 0.9557\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1246 - accuracy: 0.9670\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5001 - accuracy: 0.8542\n",
            "Test Loss: 0.5001130104064941\n",
            "Test Accuracy: 0.854200005531311\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0521 - accuracy: 0.9815\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0478 - accuracy: 0.9887\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1008 - accuracy: 0.9693\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1781 - accuracy: 0.9447\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1362 - accuracy: 0.9598\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1518 - accuracy: 0.9520\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0985 - accuracy: 0.9723\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0651 - accuracy: 0.9772\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1444 - accuracy: 0.9550\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1246 - accuracy: 0.9655\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4930 - accuracy: 0.8566\n",
            "Test Loss: 0.4929586350917816\n",
            "Test Accuracy: 0.8565999865531921\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0481 - accuracy: 0.9852\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0452 - accuracy: 0.9885\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1030 - accuracy: 0.9697\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1706 - accuracy: 0.9422\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1194 - accuracy: 0.9665\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1523 - accuracy: 0.9530\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0954 - accuracy: 0.9715\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0655 - accuracy: 0.9810\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1444 - accuracy: 0.9543\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1233 - accuracy: 0.9655\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4908 - accuracy: 0.8540\n",
            "Test Loss: 0.49079155921936035\n",
            "Test Accuracy: 0.8539999723434448\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0482 - accuracy: 0.9850\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.0473 - accuracy: 0.9880\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1030 - accuracy: 0.9680\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1681 - accuracy: 0.9433\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1244 - accuracy: 0.9640\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1606 - accuracy: 0.9505\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1025 - accuracy: 0.9695\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0676 - accuracy: 0.9780\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1396 - accuracy: 0.9552\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1264 - accuracy: 0.9680\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4857 - accuracy: 0.8581\n",
            "Test Loss: 0.48566117882728577\n",
            "Test Accuracy: 0.8580999970436096\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0480 - accuracy: 0.9847\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0412 - accuracy: 0.9892\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0925 - accuracy: 0.9730\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1600 - accuracy: 0.9448\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1173 - accuracy: 0.9645\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1580 - accuracy: 0.9488\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0900 - accuracy: 0.9740\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0690 - accuracy: 0.9790\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1470 - accuracy: 0.9512\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1180 - accuracy: 0.9687\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4863 - accuracy: 0.8558\n",
            "Test Loss: 0.48631587624549866\n",
            "Test Accuracy: 0.8557999730110168\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0515 - accuracy: 0.9837\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0380 - accuracy: 0.9902\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0934 - accuracy: 0.9708\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1552 - accuracy: 0.9503\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1190 - accuracy: 0.9638\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1575 - accuracy: 0.9492\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0997 - accuracy: 0.9720\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0660 - accuracy: 0.9792\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1418 - accuracy: 0.9533\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1207 - accuracy: 0.9683\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4773 - accuracy: 0.8590\n",
            "Test Loss: 0.47730767726898193\n",
            "Test Accuracy: 0.859000027179718\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0473 - accuracy: 0.9838\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0437 - accuracy: 0.9910\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0918 - accuracy: 0.9682\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1624 - accuracy: 0.9447\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1215 - accuracy: 0.9608\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1706 - accuracy: 0.9470\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1004 - accuracy: 0.9693\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0667 - accuracy: 0.9788\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1543 - accuracy: 0.9508\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1218 - accuracy: 0.9665\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4811 - accuracy: 0.8598\n",
            "Test Loss: 0.48106035590171814\n",
            "Test Accuracy: 0.8597999811172485\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0546 - accuracy: 0.9830\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0383 - accuracy: 0.9895\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0923 - accuracy: 0.9710\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1605 - accuracy: 0.9482\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1210 - accuracy: 0.9623\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1594 - accuracy: 0.9503\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1001 - accuracy: 0.9692\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0651 - accuracy: 0.9802\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1520 - accuracy: 0.9502\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1254 - accuracy: 0.9663\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4796 - accuracy: 0.8574\n",
            "Test Loss: 0.47956880927085876\n",
            "Test Accuracy: 0.8574000000953674\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0517 - accuracy: 0.9835\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0461 - accuracy: 0.9877\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0947 - accuracy: 0.9698\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1634 - accuracy: 0.9468\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1219 - accuracy: 0.9612\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1588 - accuracy: 0.9493\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0917 - accuracy: 0.9730\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0687 - accuracy: 0.9770\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1376 - accuracy: 0.9557\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1206 - accuracy: 0.9663\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4722 - accuracy: 0.8630\n",
            "Test Loss: 0.4722214937210083\n",
            "Test Accuracy: 0.8629999756813049\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0513 - accuracy: 0.9842\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0425 - accuracy: 0.9892\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0939 - accuracy: 0.9710\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1771 - accuracy: 0.9420\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1182 - accuracy: 0.9638\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1559 - accuracy: 0.9492\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0864 - accuracy: 0.9735\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0590 - accuracy: 0.9810\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1570 - accuracy: 0.9480\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1248 - accuracy: 0.9693\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4692 - accuracy: 0.8639\n",
            "Test Loss: 0.4691902697086334\n",
            "Test Accuracy: 0.8639000058174133\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0514 - accuracy: 0.9853\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0446 - accuracy: 0.9872\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0874 - accuracy: 0.9710\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1657 - accuracy: 0.9438\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1228 - accuracy: 0.9600\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1593 - accuracy: 0.9455\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1049 - accuracy: 0.9683\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0643 - accuracy: 0.9817\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1507 - accuracy: 0.9542\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1213 - accuracy: 0.9662\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4668 - accuracy: 0.8649\n",
            "Test Loss: 0.4667644500732422\n",
            "Test Accuracy: 0.8648999929428101\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0508 - accuracy: 0.9840\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0429 - accuracy: 0.9890\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0937 - accuracy: 0.9698\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1643 - accuracy: 0.9470\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1070 - accuracy: 0.9658\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1589 - accuracy: 0.9477\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0928 - accuracy: 0.9715\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0655 - accuracy: 0.9802\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1507 - accuracy: 0.9517\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1205 - accuracy: 0.9697\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4665 - accuracy: 0.8652\n",
            "Test Loss: 0.466521292924881\n",
            "Test Accuracy: 0.8651999831199646\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0523 - accuracy: 0.9832\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0471 - accuracy: 0.9867\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0927 - accuracy: 0.9732\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1668 - accuracy: 0.9440\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1250 - accuracy: 0.9642\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1519 - accuracy: 0.9543\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1032 - accuracy: 0.9678\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0725 - accuracy: 0.9777\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1431 - accuracy: 0.9542\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1219 - accuracy: 0.9652\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4630 - accuracy: 0.8659\n",
            "Test Loss: 0.46301206946372986\n",
            "Test Accuracy: 0.8658999800682068\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0465 - accuracy: 0.9838\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0408 - accuracy: 0.9897\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0926 - accuracy: 0.9737\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1605 - accuracy: 0.9430\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1232 - accuracy: 0.9642\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1613 - accuracy: 0.9498\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0957 - accuracy: 0.9712\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0702 - accuracy: 0.9787\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1498 - accuracy: 0.9532\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1157 - accuracy: 0.9710\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4588 - accuracy: 0.8646\n",
            "Test Loss: 0.458803653717041\n",
            "Test Accuracy: 0.8646000027656555\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0544 - accuracy: 0.9837\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0468 - accuracy: 0.9873\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0985 - accuracy: 0.9735\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1620 - accuracy: 0.9478\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1256 - accuracy: 0.9607\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1579 - accuracy: 0.9503\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1065 - accuracy: 0.9670\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0648 - accuracy: 0.9795\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1455 - accuracy: 0.9530\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1194 - accuracy: 0.9672\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4605 - accuracy: 0.8652\n",
            "Test Loss: 0.46045172214508057\n",
            "Test Accuracy: 0.8651999831199646\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0519 - accuracy: 0.9842\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0492 - accuracy: 0.9870\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0921 - accuracy: 0.9712\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1697 - accuracy: 0.9428\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1151 - accuracy: 0.9642\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1628 - accuracy: 0.9502\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1050 - accuracy: 0.9732\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0739 - accuracy: 0.9775\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1556 - accuracy: 0.9483\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1147 - accuracy: 0.9692\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4582 - accuracy: 0.8661\n",
            "Test Loss: 0.45823124051094055\n",
            "Test Accuracy: 0.866100013256073\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0486 - accuracy: 0.9853\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0441 - accuracy: 0.9903\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0912 - accuracy: 0.9702\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1647 - accuracy: 0.9470\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1160 - accuracy: 0.9657\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1708 - accuracy: 0.9455\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1072 - accuracy: 0.9695\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0742 - accuracy: 0.9760\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1522 - accuracy: 0.9517\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1259 - accuracy: 0.9658\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4565 - accuracy: 0.8657\n",
            "Test Loss: 0.4565337896347046\n",
            "Test Accuracy: 0.8657000064849854\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0513 - accuracy: 0.9832\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0483 - accuracy: 0.9893\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0945 - accuracy: 0.9708\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1635 - accuracy: 0.9460\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1229 - accuracy: 0.9605\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1556 - accuracy: 0.9487\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0992 - accuracy: 0.9702\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0731 - accuracy: 0.9778\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1615 - accuracy: 0.9465\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1272 - accuracy: 0.9682\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4596 - accuracy: 0.8641\n",
            "Test Loss: 0.4596462547779083\n",
            "Test Accuracy: 0.8640999794006348\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0489 - accuracy: 0.9850\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0443 - accuracy: 0.9882\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0966 - accuracy: 0.9723\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1632 - accuracy: 0.9465\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1240 - accuracy: 0.9643\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1595 - accuracy: 0.9475\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0990 - accuracy: 0.9725\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0723 - accuracy: 0.9750\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1485 - accuracy: 0.9500\n",
            "188/188 [==============================] - 1s 4ms/step - loss: 0.1157 - accuracy: 0.9677\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4595 - accuracy: 0.8625\n",
            "Test Loss: 0.45948731899261475\n",
            "Test Accuracy: 0.862500011920929\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0532 - accuracy: 0.9843\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0424 - accuracy: 0.9883\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0919 - accuracy: 0.9743\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1587 - accuracy: 0.9408\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1215 - accuracy: 0.9625\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1635 - accuracy: 0.9485\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1058 - accuracy: 0.9675\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0669 - accuracy: 0.9792\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1442 - accuracy: 0.9545\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1176 - accuracy: 0.9663\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4499 - accuracy: 0.8691\n",
            "Test Loss: 0.4498974680900574\n",
            "Test Accuracy: 0.8690999746322632\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0468 - accuracy: 0.9840\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0441 - accuracy: 0.9887\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.0912 - accuracy: 0.9700\n",
            "188/188 [==============================] - 2s 3ms/step - loss: 0.1624 - accuracy: 0.9437\n",
            "188/188 [==============================] - 1s 3ms/step - loss: 0.1182 - accuracy: 0.9628\n",
            "101/188 [===============>..............] - ETA: 0s - loss: 0.2321 - accuracy: 0.9233"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)\n",
        "\n",
        "import hashlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from keras.datasets import mnist\n",
        "import keras\n",
        "import random\n",
        "import time\n",
        "from random import choice\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as Ks\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.layers import Convolution2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# to get the training data, and split the data via the number of clients\n",
        "class Get_data:\n",
        "  def __init__(self,n):\n",
        "    self.n=n # number of clients\n",
        "\n",
        "  def load_data(self):\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "  def flatten_data(self,X):\n",
        "    data=[]\n",
        "    for i in X:\n",
        "      da=i.flatten()\n",
        "      data.append(da)\n",
        "    return data\n",
        "\n",
        "  def one_d_to_n_d(self,X):\n",
        "    data=[]\n",
        "    for i in X:\n",
        "      da=i.reshape(28,28)\n",
        "      data.append(da)\n",
        "    return data\n",
        "\n",
        "  def non_iid(self,X,y):\n",
        "    train_X=self.flatten_data(X)\n",
        "    train_y=list(y)\n",
        "    train_data=np.c_[train_X,train_y]\n",
        "    sort_data=train_data[np.argsort(train_data[:,784])]\n",
        "    train_x=sort_data[:,0:784]\n",
        "    train_Y=np.array(sort_data[:,784])\n",
        "    train_x_da=np.array(self.one_d_to_n_d(train_x))\n",
        "    return train_x_da,train_Y\n",
        "\n",
        "  def split_data(self, data, n): \n",
        "    size=int(len(data) / self.n)\n",
        "    s_data = []\n",
        "    for i in range(0, int(len(data)) + 1, size):\n",
        "        c_data = data[i:i + size]\n",
        "        if c_data != []:\n",
        "            s_data.append(c_data)\n",
        "    return s_data\n",
        "\n",
        "  def pre_data(self):\n",
        "\n",
        "    X_train, y_train, X_test, y_test=self.load_data()#iid\n",
        "\n",
        "    X_train,y_train=self.non_iid(X_train,y_train) # non_iid\n",
        "\n",
        "    X_train=self.split_data(X_train,self.n) \n",
        "    y_train=self.split_data(y_train,self.n)\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "class Model:\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def global_models(self):\n",
        "    model= tf.keras.models.Sequential([\n",
        "          tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "          tf.keras.layers.Dense(128, activation='relu'),\n",
        "          tf.keras.layers.Dropout(0.2),\n",
        "          tf.keras.layers.Dense(10, activation='softmax')\n",
        "      ])\n",
        "    return model\n",
        "  \n",
        "  def global_model(self):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(28,28,1))) \n",
        "    model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "  def evaluate_model(self,model,test_X, test_y):\n",
        "    model.compile(optimizer='sgd',\n",
        "                  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "                  metrics=['accuracy'])\n",
        "    score=model.evaluate(test_X, test_y)\n",
        "    print(\"Test Loss:\", score[0])\n",
        "    print('Test Accuracy:', score[1])\n",
        "    return score[0],score[1]\n",
        "\n",
        "class Client:\n",
        "  \n",
        "  def __init__(self,lr,epoch):\n",
        "    #self.n=n\n",
        "    self.epoch=epoch\n",
        "    self.lr = lr\n",
        "    self.loss='categorical_crossentropy'\n",
        "    self.metrics = ['accuracy']\n",
        "    self.optimizer = SGD(lr=self.lr, \n",
        "                decay=self.lr / 2, \n",
        "                momentum=0.9\n",
        "               )\n",
        "  def weight_client(self,data,m,n):\n",
        "    wei_client = []\n",
        "    for i in range(n):\n",
        "        len_data = len(data[i])\n",
        "        proba = len_data / m\n",
        "        wei_client.append(proba)\n",
        "    return wei_client\n",
        "    \n",
        "  \n",
        "  def scale_model_weights(self,weight,scalar,num):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "\n",
        "    fac=scalar[num]\n",
        "\n",
        "    sca=[fac for i in range(steps)]\n",
        "\n",
        "    for i in range(steps):\n",
        "      weight_final.append(sca[i]*weight[i])\n",
        "        #weight_final.append(weight[i])\n",
        "    return weight_final\n",
        "\n",
        "  def training(self,X,y,global_weights):\n",
        "\n",
        "    #fact=self.weight_client()\n",
        "    model=Model().global_model()\n",
        "\n",
        "    model.compile(optimizer=self.optimizer,\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    model.set_weights(global_weights)\n",
        "    model.fit(X,y,epochs=self.epoch)\n",
        "    weights=model.get_weights()\n",
        "\n",
        "    return weights\n",
        "\n",
        "class Server:\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def scale_model_weights(self,weight,scalar,num):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    wei=[]\n",
        "    steps = 8\n",
        "\n",
        "    for j in range(num):\n",
        "      fa=scalar[j]\n",
        "      scar=[fa for k in range(steps)]\n",
        "      for i in range(steps):\n",
        "        weight_final.append(scar[i]*weight[j][i])\n",
        "        #weight_final.append(weight[i])\n",
        "      wei.append(weight_final)\n",
        "    return wei\n",
        "\n",
        "  def sum_scaled_weights(self,scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "    return avg_grad\n",
        "\n",
        "  def evaluate(self,model,test_X, test_y):\n",
        "    loss,acc=Model.evaluate_model(model,test_X, test_y)\n",
        "    return loss,acc\n",
        "\n",
        "def main(num_client,K,lr,epoch):\n",
        "  client=Client(lr,epoch)\n",
        "  get_data=Get_data(num_client)\n",
        "  train_X,train_y, test_X, test_y=get_data.load_data()\n",
        "  m=len(train_X)\n",
        "\n",
        "  X_train, y_train, X_test, y_test=get_data.pre_data()\n",
        "\n",
        "  server=Server()\n",
        "  global_model=Model().global_model()\n",
        "\n",
        "  accuracy=[]\n",
        "  losses=[]\n",
        "  factor=[0.1 for i in range(10)]\n",
        "  for k in range(K):\n",
        "    print(k)\n",
        "    global_weights=global_model.get_weights()\n",
        "    weit=[]\n",
        "    weitt=[]\n",
        "    for i in range(num_client):\n",
        "      weix=client.training(X_train[i], y_train[i],global_weights)\n",
        "      weix=client.scale_model_weights(weix,factor,i)\n",
        "      weit.append(weix)\n",
        "    global_weight=server.sum_scaled_weights(weit) # fedavg\n",
        "    global_model.set_weights(global_weight)\n",
        "    loss,acc=Model().evaluate_model(global_model,test_X,test_y)\n",
        "    losses.append(loss)\n",
        "    accuracy.append(acc)\n",
        "  return losses,accuracy\n",
        "\n",
        "if __name__=='__main__':\n",
        "  K=100 # number of local rounds\n",
        "  num_client=10# number of clients for each server\n",
        "  lr=0.001 # learning rate\n",
        "  epoch=1 # local iterations\n",
        "\n",
        "  loss,acc=main(num_client,K,lr,epoch)\n",
        "  loss\n",
        "  print('====================================loss================================')\n",
        "  print(loss)\n",
        "  print('====================================acc================================')\n",
        "  print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sve-j1Aj9Hlp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}