{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fl_mnist_iid_faker.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMtV6okliuVi2DmaoSBMCsY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wzljerry/Hierarchical-Federated-Learning/blob/main/fl_mnist_iid_faker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbMRJvvCaFYG",
        "outputId": "af187983-46a1-4385-d6e7-60c41e1a5c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py:1768: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:77: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:77: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:1082: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a sigmoid or softmax activation and thus does not represent logits. Was this intended?\"\n",
            "  return dispatch_target(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 6.0090 - accuracy: 0.6425\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 5.9103 - accuracy: 0.6022\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 6.0110 - accuracy: 0.5626\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 6.0940 - accuracy: 0.5982\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 5.8275 - accuracy: 0.6654\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.2833 - accuracy: 0.6030\n",
            "Test Loss: 1.2832720279693604\n",
            "Test Accuracy: 0.6029999852180481\n",
            "1\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.1586 - accuracy: 0.6706\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.2215 - accuracy: 0.6653\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.2599 - accuracy: 0.6527\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.2412 - accuracy: 0.6564\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.1249 - accuracy: 0.6906\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.8105 - accuracy: 0.7756\n",
            "Test Loss: 0.8105104565620422\n",
            "Test Accuracy: 0.775600016117096\n",
            "2\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9296 - accuracy: 0.7412\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9655 - accuracy: 0.7247\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 1.0372 - accuracy: 0.7168\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9674 - accuracy: 0.7321\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9026 - accuracy: 0.7456\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.6849 - accuracy: 0.8165\n",
            "Test Loss: 0.6848520040512085\n",
            "Test Accuracy: 0.8165000081062317\n",
            "3\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8124 - accuracy: 0.7730\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8654 - accuracy: 0.7566\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.9029 - accuracy: 0.7523\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8584 - accuracy: 0.7537\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.7767 - accuracy: 0.7846\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.6155 - accuracy: 0.8360\n",
            "Test Loss: 0.615547776222229\n",
            "Test Accuracy: 0.8360000252723694\n",
            "4\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.7366 - accuracy: 0.7920\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.7785 - accuracy: 0.7776\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.8429 - accuracy: 0.7707\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.7829 - accuracy: 0.7798\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.6978 - accuracy: 0.8015\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5675 - accuracy: 0.8506\n",
            "Test Loss: 0.5675297975540161\n",
            "Test Accuracy: 0.850600004196167\n",
            "5\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.6925 - accuracy: 0.8074\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.7130 - accuracy: 0.7952\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.7561 - accuracy: 0.7939\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.7264 - accuracy: 0.7910\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.6396 - accuracy: 0.8165\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.5261 - accuracy: 0.8615\n",
            "Test Loss: 0.5260995626449585\n",
            "Test Accuracy: 0.8615000247955322\n",
            "6\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.6291 - accuracy: 0.8173\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.6794 - accuracy: 0.7998\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.7039 - accuracy: 0.8065\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.6673 - accuracy: 0.8087\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.6042 - accuracy: 0.8262\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4913 - accuracy: 0.8675\n",
            "Test Loss: 0.49132415652275085\n",
            "Test Accuracy: 0.8675000071525574\n",
            "7\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.6024 - accuracy: 0.8232\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.6374 - accuracy: 0.8134\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.6630 - accuracy: 0.8108\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.6544 - accuracy: 0.8133\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.5682 - accuracy: 0.8363\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4708 - accuracy: 0.8759\n",
            "Test Loss: 0.47080934047698975\n",
            "Test Accuracy: 0.8758999705314636\n",
            "8\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.5802 - accuracy: 0.8315\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.6042 - accuracy: 0.8210\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.6391 - accuracy: 0.8188\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.6225 - accuracy: 0.8184\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.5632 - accuracy: 0.8349\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4529 - accuracy: 0.8765\n",
            "Test Loss: 0.4529190957546234\n",
            "Test Accuracy: 0.8765000104904175\n",
            "9\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.5612 - accuracy: 0.8368\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.5830 - accuracy: 0.8306\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.6027 - accuracy: 0.8267\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.5964 - accuracy: 0.8301\n",
            "375/375 [==============================] - 1s 2ms/step - loss: 0.5357 - accuracy: 0.8397\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.4332 - accuracy: 0.8855\n",
            "Test Loss: 0.43316811323165894\n",
            "Test Accuracy: 0.8855000138282776\n",
            "====================================loss================================\n",
            "[1.2832720279693604, 0.8105104565620422, 0.6848520040512085, 0.615547776222229, 0.5675297975540161, 0.5260995626449585, 0.49132415652275085, 0.47080934047698975, 0.4529190957546234, 0.43316811323165894]\n",
            "====================================acc================================\n",
            "[0.6029999852180481, 0.775600016117096, 0.8165000081062317, 0.8360000252723694, 0.850600004196167, 0.8615000247955322, 0.8675000071525574, 0.8758999705314636, 0.8765000104904175, 0.8855000138282776]\n"
          ]
        }
      ],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "'''\n",
        "@Title : Faker\n",
        "@Author : Zhilin Wang\n",
        "@Email : wangzhil.edu\n",
        "@Date : 8-08-2022\n",
        "'''\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)\n",
        "\n",
        "import hashlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from keras.datasets import mnist\n",
        "import keras\n",
        "import random\n",
        "import time\n",
        "from random import choice\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as Ks\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.layers import Convolution2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# to get the training data, and split the data via the number of clients\n",
        "class Get_data:\n",
        "  def __init__(self,n):\n",
        "    self.n=n # number of clients\n",
        "\n",
        "  def load_data(self):\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "  def flatten_data(self,X):\n",
        "    data=[]\n",
        "    for i in X:\n",
        "      da=i.flatten()\n",
        "      data.append(da)\n",
        "    return data\n",
        "\n",
        "  def one_d_to_n_d(self,X):\n",
        "    data=[]\n",
        "    for i in X:\n",
        "      da=i.reshape(28,28)\n",
        "      data.append(da)\n",
        "    return data\n",
        "\n",
        "  def non_iid(self,X,y):\n",
        "    train_X=self.flatten_data(X)\n",
        "    train_y=list(y)\n",
        "    train_data=np.c_[train_X,train_y]\n",
        "    sort_data=train_data[np.argsort(train_data[:,784])]\n",
        "    train_x=sort_data[:,0:784]\n",
        "    train_Y=np.array(sort_data[:,784])\n",
        "    train_x_da=np.array(self.one_d_to_n_d(train_x))\n",
        "    return train_x_da,train_Y\n",
        "\n",
        "  def split_data(self, data, n): \n",
        "    size=int(len(data) / self.n)\n",
        "    s_data = []\n",
        "    for i in range(0, int(len(data)) + 1, size):\n",
        "        c_data = data[i:i + size]\n",
        "        if c_data != []:\n",
        "            s_data.append(c_data)\n",
        "    return s_data\n",
        "\n",
        "  def pre_data(self):\n",
        "\n",
        "    X_train, y_train, X_test, y_test=self.load_data()#iid\n",
        "\n",
        "    #X_train,y_train=self.non_iid(X_train,y_train) # non_iid\n",
        "\n",
        "    X_train=self.split_data(X_train,self.n) \n",
        "    y_train=self.split_data(y_train,self.n)\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "class Model:\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def global_models(self):\n",
        "    model= tf.keras.models.Sequential([\n",
        "          tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "          tf.keras.layers.Dense(128, activation='relu'),\n",
        "          tf.keras.layers.Dropout(0.2),\n",
        "          tf.keras.layers.Dense(10, activation='softmax')\n",
        "      ])\n",
        "    return model\n",
        "  \n",
        "  def global_model(self):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(28,28,1))) \n",
        "    model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "  def evaluate_model(self,model,test_X, test_y):\n",
        "    model.compile(optimizer='sgd',\n",
        "                  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "                  metrics=['accuracy'])\n",
        "    score=model.evaluate(test_X, test_y)\n",
        "    print(\"Test Loss:\", score[0])\n",
        "    print('Test Accuracy:', score[1])\n",
        "    return score[0],score[1]\n",
        "\n",
        "class Client:\n",
        "  \n",
        "  def __init__(self,lr,epoch):\n",
        "    #self.n=n\n",
        "    self.epoch=epoch\n",
        "    self.lr = lr\n",
        "    self.loss='categorical_crossentropy'\n",
        "    self.metrics = ['accuracy']\n",
        "    self.optimizer = SGD(lr=self.lr, \n",
        "                decay=self.lr / 2, \n",
        "                momentum=0.9\n",
        "               )\n",
        "  def weight_client(self,data,m,n):\n",
        "    wei_client = []\n",
        "    for i in range(n):\n",
        "        len_data = len(data[i])\n",
        "        proba = len_data / m\n",
        "        wei_client.append(proba)\n",
        "    return wei_client\n",
        "    \n",
        "  \n",
        "  def scale_model_weights(self,weight,scalar,num):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "\n",
        "    fac=scalar[num]\n",
        "\n",
        "    sca=[fac for i in range(steps)]\n",
        "\n",
        "    for i in range(steps):\n",
        "      weight_final.append(sca[i]*weight[i])\n",
        "        #weight_final.append(weight[i])\n",
        "    return weight_final\n",
        "\n",
        "  def training(self,X,y,global_weights):\n",
        "\n",
        "    #fact=self.weight_client()\n",
        "    model=Model().global_models()\n",
        "\n",
        "    model.compile(optimizer=self.optimizer,\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    model.set_weights(global_weights)\n",
        "    model.fit(X,y,epochs=self.epoch)\n",
        "    weights=model.get_weights()\n",
        "\n",
        "    return weights\n",
        "\n",
        "class Server:\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def scale_model_weights(self,weight,scalar,num):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    wei=[]\n",
        "    steps = 8\n",
        "\n",
        "    for j in range(num):\n",
        "      fa=scalar[j]\n",
        "      scar=[fa for k in range(steps)]\n",
        "      for i in range(steps):\n",
        "        weight_final.append(scar[i]*weight[j][i])\n",
        "        #weight_final.append(weight[i])\n",
        "      wei.append(weight_final)\n",
        "    return wei\n",
        "\n",
        "  def sum_scaled_weights(self,scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "    return avg_grad\n",
        "\n",
        "  def evaluate(self,model,test_X, test_y):\n",
        "    loss,acc=Model.evaluate_model(model,test_X, test_y)\n",
        "    return loss,acc\n",
        "\n",
        "def main(num_client,K,lr,epoch):\n",
        "  client=Client(lr,epoch)\n",
        "  get_data=Get_data(num_client)\n",
        "  X_train, y_train, X_test, y_test=get_data.pre_data()\n",
        "\n",
        "  server=Server()\n",
        "  global_model=Model().global_models()\n",
        "  accuracy=[]\n",
        "  losses=[]\n",
        "  factor=[1/num_client for i in range(num_client)]\n",
        "  fa=[1/3 for i in range(num_client)]\n",
        "  for k in range(K):\n",
        "    print(k)\n",
        "    global_weights=global_model.get_weights()\n",
        "    weit=[]\n",
        "    we=[]\n",
        "    for i in range(num_client):\n",
        "      weix=client.training(X_train[i], y_train[i],global_weights)\n",
        "      weix=client.scale_model_weights(weix,factor,i)\n",
        "      weit.append(weix)\n",
        "    global_weight=server.sum_scaled_weights(weit) # fedavg\n",
        "    global_model.set_weights(global_weight)\n",
        "    g_weit=global_model.get_weights()\n",
        "    loss,acc=Model().evaluate_model(global_model,X_test, y_test)\n",
        "    losses.append(loss)\n",
        "    accuracy.append(acc)\n",
        "  return losses,accuracy,weit,g_weit\n",
        "\n",
        "if __name__=='__main__':\n",
        "  K=10 # number of local rounds\n",
        "  num_client=5# number of clients for each server\n",
        "  lr=0.0001 # learning rate\n",
        "  epoch=1 # local iterations\n",
        "\n",
        "  loss,acc,weit,global_weight=main(num_client,K,lr,epoch)\n",
        "  loss\n",
        "  print('====================================loss================================')\n",
        "  print(loss)\n",
        "  print('====================================acc================================')\n",
        "  print(acc)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "class cosine_similarity:\n",
        "  '''\n",
        "  This classs is used to get the updated global model updates based on cosine similarities\n",
        "  weit: the list of all the weights\n",
        "  global_w: global model\n",
        "\n",
        "  '''\n",
        "  def __init__(self,weit,global_w):\n",
        "    self.weit=weit\n",
        "    self.global_w=global_w\n",
        "\n",
        "  def similarity(self,w_1,w_2):\n",
        "    '''get the similarity of each layer'''\n",
        "    sim=[]\n",
        "    simi=torch.cosine_similarity(torch.Tensor(w_1), torch.Tensor(w_2), dim=0)\n",
        "    return simi\n",
        "\n",
        "  def get_similarity(self,w_1,w_2):\n",
        "    '''get the similarity between global weights and one local weights'''\n",
        "    sim=[]\n",
        "    for i in range(len(w_1)):\n",
        "      similarit=self.similarity(w_1[i],w_2[i])\n",
        "      sim.append(similarit)\n",
        "    return sim\n",
        "      \n",
        "  def get_mean_weight(self,weit):\n",
        "    '''get the mean value of all tensors in one list'''\n",
        "    mean_weit=[]\n",
        "    for i in range(len(weit)):\n",
        "      mean_w=torch.mean(weit[i])\n",
        "      mean_weit.append(mean_w)\n",
        "      mean_weights=np.mean(mean_weit)\n",
        "    return mean_weights\n",
        "\n",
        "  def get_mean_weights_all(self):\n",
        "    '''get the mean value of all weithts compared to global weights'''\n",
        "    sim=[]\n",
        "    for i in self.weit:\n",
        "      simt=self.get_similarity(self.global_w,i)\n",
        "      simi=self.get_mean_weight(simt)\n",
        "      sim.append(simi)\n",
        "    return sim\n",
        "\n",
        "  def select_weights(self):\n",
        "    similarities=self.get_mean_weights_all()\n",
        "    max_index=similarities.index(max(similarities))\n",
        "    max_weights=self.weit[max_index]\n",
        "    return max_index, max_weights\n",
        "\n",
        "\n",
        "\n",
        "cos=cosine_similarity(weit,global_weight)\n",
        "mean_weights=cos.get_mean_weights_all()\n",
        "print(mean_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAntuyXOc-ej",
        "outputId": "42b739c4-e3a2-4098-a644-5cdd8eac852d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.9999127, 0.9998889, 0.99990386, 0.99990934, 0.9999082]\n"
          ]
        }
      ]
    }
  ]
}