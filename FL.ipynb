{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FL.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO8WinSiAspv29dGZ4/fq2f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wzljerry/Hierarchical-Federated-Learning/blob/main/FL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0GhRZ5pq57qx"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "'''\n",
        "@Title : Traditional Federated Learning\n",
        "@Author : Zhilin Wang\n",
        "@Email : wangzhil.edu\n",
        "@Date : 3-08-2022\n",
        "@Reference: https://towardsdatascience.com/federated-learning-a-step-by-step-implementation-in-tensorflow-aac568283399 \n",
        "'''\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)\n",
        "\n",
        "import hashlib\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from keras.datasets import mnist\n",
        "import keras\n",
        "import random\n",
        "import time\n",
        "from random import choice\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras import backend as Ks\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from keras.layers import Convolution2D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "# to get the training data, and split the data via the number of clients\n",
        "class Get_data:\n",
        "  def __init__(self,n):\n",
        "    self.n=n # number of clients\n",
        "\n",
        "  def load_data(self):\n",
        "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "  def flatten_data(self,X):\n",
        "    data=[]\n",
        "    for i in X:\n",
        "      da=i.flatten()\n",
        "      data.append(da)\n",
        "    return data\n",
        "\n",
        "  def one_d_to_n_d(self,X):\n",
        "    data=[]\n",
        "    for i in X:\n",
        "      da=i.reshape(28,28)\n",
        "      data.append(da)\n",
        "    return data\n",
        "\n",
        "  def non_iid(self,X,y):\n",
        "    train_X=self.flatten_data(X)\n",
        "    train_y=list(y)\n",
        "    train_data=np.c_[train_X,train_y]\n",
        "    sort_data=train_data[np.argsort(train_data[:,784])]\n",
        "    train_x=sort_data[:,0:784]\n",
        "    train_Y=np.array(sort_data[:,784])\n",
        "    train_x_da=np.array(self.one_d_to_n_d(train_x))\n",
        "    return train_x_da,train_Y\n",
        "\n",
        "  def split_data(self, data, n): \n",
        "    size=int(len(data) / self.n)\n",
        "    s_data = []\n",
        "    for i in range(0, int(len(data)) + 1, size):\n",
        "        c_data = data[i:i + size]\n",
        "        if c_data != []:\n",
        "            s_data.append(c_data)\n",
        "    return s_data\n",
        "\n",
        "  def pre_data(self):\n",
        "\n",
        "    X_train, y_train, X_test, y_test=self.load_data()#iid\n",
        "\n",
        "    X_train,y_train=self.non_iid(X_train,y_train) # non_iid\n",
        "\n",
        "    X_train=self.split_data(X_train,self.n) \n",
        "    y_train=self.split_data(y_train,self.n)\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "class Model:\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def global_models(self):\n",
        "    model= tf.keras.models.Sequential([\n",
        "          tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "          tf.keras.layers.Dense(128, activation='relu'),\n",
        "          tf.keras.layers.Dropout(0.2),\n",
        "          tf.keras.layers.Dense(10, activation='softmax')\n",
        "      ])\n",
        "    return model\n",
        "  \n",
        "  def global_model(self):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(28,28,1))) \n",
        "    model.add(Convolution2D(32, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.25))\n",
        "\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "    return model\n",
        "\n",
        "  def evaluate_model(self,model,test_X, test_y):\n",
        "    model.compile(optimizer='sgd',\n",
        "                  loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
        "                  metrics=['accuracy'])\n",
        "    score=model.evaluate(test_X, test_y)\n",
        "    print(\"Test Loss:\", score[0])\n",
        "    print('Test Accuracy:', score[1])\n",
        "    return score[0],score[1]\n",
        "\n",
        "class Client:\n",
        "  \n",
        "  def __init__(self,lr,epoch):\n",
        "    #self.n=n\n",
        "    self.epoch=epoch\n",
        "    self.lr = lr\n",
        "    self.loss='categorical_crossentropy'\n",
        "    self.metrics = ['accuracy']\n",
        "    self.optimizer = SGD(lr=self.lr, \n",
        "                decay=self.lr / 2, \n",
        "                momentum=0.9\n",
        "               )\n",
        "  def weight_client(self,data,m,n):\n",
        "    wei_client = []\n",
        "    for i in range(n):\n",
        "        len_data = len(data[i])\n",
        "        proba = len_data / m\n",
        "        wei_client.append(proba)\n",
        "    return wei_client\n",
        "    \n",
        "  \n",
        "  def scale_model_weights(self,weight,scalar,num):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    steps = len(weight)\n",
        "\n",
        "    fac=scalar[num]\n",
        "\n",
        "    sca=[fac for i in range(steps)]\n",
        "\n",
        "    for i in range(steps):\n",
        "      weight_final.append(sca[i]*weight[i])\n",
        "        #weight_final.append(weight[i])\n",
        "    return weight_final\n",
        "\n",
        "  def training(self,X,y,global_weights):\n",
        "\n",
        "    #fact=self.weight_client()\n",
        "    model=Model().global_model()\n",
        "\n",
        "    model.compile(optimizer=self.optimizer,\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "    \n",
        "    model.set_weights(global_weights)\n",
        "    model.fit(X,y,epochs=self.epoch)\n",
        "    weights=model.get_weights()\n",
        "\n",
        "    return weights\n",
        "\n",
        "class Server:\n",
        "  \n",
        "  def __init__(self):\n",
        "    pass\n",
        "  \n",
        "  def scale_model_weights(self,weight,scalar,num):\n",
        "    '''function for scaling a models weights'''\n",
        "    weight_final = []\n",
        "    wei=[]\n",
        "    steps = 8\n",
        "\n",
        "    for j in range(num):\n",
        "      fa=scalar[j]\n",
        "      scar=[fa for k in range(steps)]\n",
        "      for i in range(steps):\n",
        "        weight_final.append(scar[i]*weight[j][i])\n",
        "        #weight_final.append(weight[i])\n",
        "      wei.append(weight_final)\n",
        "    return wei\n",
        "\n",
        "  def sum_scaled_weights(self,scaled_weight_list):\n",
        "    '''Return the sum of the listed scaled weights. The is equivalent to scaled avg of the weights'''\n",
        "    avg_grad = list()\n",
        "    #get the average grad accross all client gradients\n",
        "    for grad_list_tuple in zip(*scaled_weight_list):\n",
        "        layer_mean = tf.math.reduce_sum(grad_list_tuple, axis=0)\n",
        "        avg_grad.append(layer_mean)\n",
        "    return avg_grad\n",
        "\n",
        "  def evaluate(self,model,test_X, test_y):\n",
        "    loss,acc=Model.evaluate_model(model,test_X, test_y)\n",
        "    return loss,acc\n",
        "\n",
        "def main(num_client,K,lr,epoch):\n",
        "  client=Client(lr,epoch)\n",
        "  get_data=Get_data(num_client)\n",
        "  train_X,train_y, test_X, test_y=get_data.load_data()\n",
        "  m=len(train_X)\n",
        "\n",
        "  X_train, y_train, X_test, y_test=get_data.pre_data()\n",
        "\n",
        "  server=Server()\n",
        "  global_model=Model().global_model()\n",
        "\n",
        "  accuracy=[]\n",
        "  losses=[]\n",
        "  factor=[0.1 for i in range(10)]\n",
        "  for k in range(K):\n",
        "    print(k)\n",
        "    global_weights=global_model.get_weights()\n",
        "    weit=[]\n",
        "    weitt=[]\n",
        "    for i in range(num_client):\n",
        "      weix=client.training(X_train[i], y_train[i],global_weights)\n",
        "      weix=client.scale_model_weights(weix,factor,i)\n",
        "      weit.append(weix)\n",
        "    global_weight=server.sum_scaled_weights(weit) # fedavg\n",
        "    global_model.set_weights(global_weight)\n",
        "    loss,acc=Model().evaluate_model(global_model,test_X,test_y)\n",
        "    losses.append(loss)\n",
        "    accuracy.append(acc)\n",
        "  return losses,accuracy\n",
        "\n",
        "if __name__=='__main__':\n",
        "  K=100 # number of local rounds\n",
        "  num_client=10# number of clients for each server\n",
        "  lr=0.001 # learning rate\n",
        "  epoch=1 # local iterations\n",
        "\n",
        "  loss,acc=main(num_client,K,lr,epoch)\n",
        "  loss\n",
        "  print('====================================loss================================')\n",
        "  print(loss)\n",
        "  print('====================================acc================================')\n",
        "  print(acc)"
      ]
    }
  ]
}